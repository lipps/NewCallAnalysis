# 批量文件处理功能开发任务计划

## 📋 总体目标
实现多文件批量上传、分析、下载的完整功能，支持JSON/CSV/TXT格式，提供进度跟踪和错误处理机制。

## 🏗️ 架构分析

### 现有基础设施 ✅
- **工作流引擎**: `SimpleCallAnalysisWorkflow.execute_batch()` 已实现并发批处理
- **API基础**: 已有 `/analyze/batch` 异步接口，缺乏同步版本和结果查询
- **前端框架**: Streamlit `render_batch_analysis()` 已有文件上传组件
- **数据模型**: `CallInput`/`CallAnalysisResult` 完善，需补充批量文件相关模型

### 关键缺口 ❌
- 缺少文件解析标准化工具
- 缺少同步批量API接口
- 缺少按文件分组的结果处理逻辑
- 缺少前端进度展示和下载功能

## 📊 执行计划（按优先级和依赖关系）

### 第一阶段：核心基础设施 (1-2天)

#### Task 1.1: 数据模型扩展 [优先级: 高] [依赖: 无]
**文件**: `src/models/schemas.py`
```python
# 需要新增的模型
class BatchFileInput(BaseModel)
class BatchFileResult(BaseModel)
class BatchFileResponse(BaseModel)
class FileParseResult(BaseModel)
```
- [ ] 定义 `BatchFileInput`: 包含 source_name, calls, parse_status
- [ ] 定义 `BatchFileResult`: 包含 status, result, metrics, error
- [ ] 定义 `BatchFileResponse`: 包含 files 列表和 statistics 汇总
- [ ] 扩展 `AnalysisConfig` 添加批量处理参数
- [ ] 编写单元测试验证模型序列化

#### Task 1.2: 文件解析工具模块 [优先级: 高] [依赖: 1.1]
**文件**: `src/utils/file_parser.py` (新建)
- [ ] 实现 JSON 文件解析器 (支持多种格式)
  - `List[CallInput]`
  - `{"calls": [...]}`
  - 单个 `CallInput` 对象
- [ ] 实现 CSV 文件解析器 (至少包含 transcript 列)
- [ ] 实现 TXT 文件解析器 (一文件一通话)
- [ ] 统一错误处理和日志记录
- [ ] 编写解析器单元测试

#### Task 1.3: 配置扩展 [优先级: 中] [依赖: 无] [可并行]
**文件**: `src/config/settings.py`
- [ ] 添加批量处理配置项
  ```python
  BATCH_MAX_FILES=20
  BATCH_MAX_CALLS=2000
  BATCH_CONTINUE_ON_ERROR=true
  BATCH_RESULT_STORAGE=LOCAL|MEMORY
  ```
- [ ] 更新 `.env.example` 添加新配置说明

### 第二阶段：API层实现 (2-3天)

#### Task 2.1: 同步批量分析API [优先级: 高] [依赖: 1.1, 1.2]
**文件**: `src/api/main.py`
- [ ] 实现 `POST /analyze/batch/sync` 接口
- [ ] 集成文件解析工具
- [ ] 实现按文件分组的分析逻辑
- [ ] 添加统计信息收集
- [ ] 实现错误隔离 (单文件失败不影响其他)
- [ ] 添加请求验证 (文件大小、数量限制)

#### Task 2.2: 结果持久化选项 [优先级: 中] [依赖: 2.1] [可并行]
**文件**: `src/api/storage.py` (新建)
- [ ] 实现本地文件存储 (可选)
- [ ] 结果文件清理机制
- [ ] 存储路径管理

#### Task 2.3: API集成测试 [优先级: 中] [依赖: 2.1]
**文件**: `tests/test_batch_api.py` (新建)
- [ ] 测试正常批量分析流程
- [ ] 测试文件解析失败场景
- [ ] 测试部分成功/失败混合场景
- [ ] 性能测试 (大文件处理)

### 第三阶段：前端实现 (2-3天)

#### Task 3.1: 批量分析页面重构 [优先级: 高] [依赖: 2.1]
**文件**: `src/dashboard/streamlit_app.py`
- [ ] 重写 `render_batch_analysis()` 方法
- [ ] 实现文件上传后的预览功能
- [ ] 集成文件解析预检
- [ ] 添加配置选项 (最大并发、引擎开关)

#### Task 3.2: 进度跟踪和结果展示 [优先级: 高] [依赖: 3.1]
**文件**: `src/dashboard/streamlit_app.py`
- [ ] 实现调用 API 的异步处理
- [ ] 添加 `st.progress` 进度条
- [ ] 实现单文件结果卡片展示
- [ ] 添加成功/失败状态指示

#### Task 3.3: 结果下载功能 [优先级: 高] [依赖: 3.2]
**文件**: `src/dashboard/streamlit_app.py`
- [ ] 实现按文件的结果下载
- [ ] 生成汇总报告下载
- [ ] 文件命名去重处理
- [ ] 下载按钮状态管理

#### Task 3.4: 错误处理和重试 [优先级: 中] [依赖: 3.2] [可并行]
**文件**: `src/dashboard/streamlit_app.py`
- [ ] 失败文件的错误信息展示
- [ ] 单文件重新分析功能
- [ ] 用户友好的错误提示

### 第四阶段：优化和测试 (1-2天)

#### Task 4.1: 端到端测试 [优先级: 高] [依赖: 3.3]
- [ ] 准备测试数据集 (正常/异常文件各若干)
- [ ] 完整流程测试: 上传→解析→分析→下载
- [ ] 边界情况测试: 空文件、大文件、格式错误等
- [ ] 性能基准测试

#### Task 4.2: 用户体验优化 [优先级: 中] [依赖: 4.1]
- [ ] 添加操作指导和帮助文档
- [ ] 优化加载状态和反馈信息
- [ ] 响应式设计调整
- [ ] 键盘快捷键支持

#### Task 4.3: 监控和日志完善 [优先级: 中] [依赖: 4.1] [可并行]
- [ ] 完善批量处理日志格式
- [ ] 添加性能监控点
- [ ] 错误报警机制

### 第五阶段：文档和部署 (0.5-1天)

#### Task 5.1: 文档更新 [优先级: 中] [可并行进行]
- [ ] 更新 `README.md` 添加批量处理说明
- [ ] 更新 API 文档
- [ ] 编写用户操作指南

#### Task 5.2: 部署脚本优化 [优先级: 低] [可并行进行]
- [ ] 更新 Docker 配置支持批量处理
- [ ] 环境变量配置检查

## 🔄 并行开发建议

### 可并行执行的任务组合:
1. **配置扩展 (Task 1.3)** 可与 **模型扩展 (Task 1.1)** 并行
2. **结果持久化 (Task 2.2)** 可与 **API测试编写 (Task 2.3)** 并行
3. **错误处理 (Task 3.4)** 可与 **结果展示 (Task 3.2)** 并行
4. **监控日志 (Task 4.3)** 可与 **端到端测试 (Task 4.1)** 并行
5. **文档工作 (Task 5.1, 5.2)** 可在开发过程中持续进行

## ⚠️ 风险点和注意事项

### 技术风险:
1. **内存占用**: 大量文件同时处理可能导致内存溢出
   - **缓解措施**: 实现流式处理，限制并发数量
2. **API超时**: 同步接口处理大批量时可能超时
   - **缓解措施**: 合理设置超时时间，提供异步选项
3. **文件格式兼容性**: 各种文件格式解析的边界情况
   - **缓解措施**: 充分的单元测试和错误处理

### 业务风险:
1. **用户体验**: 长时间等待可能导致用户流失
   - **缓解措施**: 清晰的进度指示和预估时间
2. **数据丢失**: 处理过程中的异常可能导致结果丢失
   - **缓解措施**: 实现结果缓存和恢复机制

## 📈 验收标准

### 功能验收:
- [ ] 支持 JSON/CSV/TXT 三种文件格式上传
- [ ] 单次可处理 ≤20 个文件, ≤2000 条通话记录
- [ ] 文件解析成功率 ≥95%
- [ ] 分析结果按源文件独立下载
- [ ] 失败文件提供明确错误信息和重试选项

### 性能验收:
- [ ] 单个 100 条通话记录的文件处理时间 ≤30秒
- [ ] 内存占用增长 ≤500MB (相比单次分析)
- [ ] 并发处理不影响系统稳定性

### 用户体验验收:
- [ ] 上传后即时显示文件解析状态
- [ ] 分析过程中显示实时进度
- [ ] 结果展示清晰，支持快速查看关键指标
- [ ] 错误信息用户友好，提供解决建议

## 🎯 里程碑计划

| 里程碑 | 完成时间 | 关键成果 |
|--------|----------|----------|
| M1: 基础设施完成 | Day 2 | 数据模型、文件解析器就绪 |
| M2: API 功能完成 | Day 5 | 同步批量分析接口可用 |
| M3: 前端功能完成 | Day 8 | 完整的上传-分析-下载流程 |
| M4: 测试和优化完成 | Day 10 | 通过所有验收标准 |
| M5: 文档和部署就绪 | Day 11 | 可正式发布使用 |

---

## 📝 开发者注意事项

1. **代码风格**: 保持与现有代码风格一致，遵循项目的 linting 规则
2. **错误处理**: 所有异常都应该有适当的日志记录和用户友好的错误信息
3. **测试覆盖**: 新增功能需要有对应的单元测试和集成测试
4. **向后兼容**: 确保不影响现有的单次分析功能
5. **国际化**: 所有用户面向的文本都应该支持中文显示
6. **安全考虑**: 文件上传需要验证文件类型和大小，防止恶意文件

## 🔧 开发环境准备

```bash
# 确保开发环境就绪
pip install -r requirements.txt
# 运行现有测试确保基础功能正常
pytest tests/ -v
# 启动开发服务器测试现有功能
python main.py server
python main.py dashboard
```

---
*此计划基于需求文档 `batchFileProcessReq_codexIDE.md` 制定，如需调整请及时更新本文档。*